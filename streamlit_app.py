import streamlit as st
from google import genai
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch, Content, Part
import pandas as pd
import json
import glob
import requests

# to update
after = '2025-04-01'
price = {
    'gemini-2.0-flash': {'input': 0.1, 'output': 0.4},
    'gemini-2.5-flash-preview-04-17': {'input': 0.15, 'output': 0.6},
}

prompt_token_count = 0
candidates_token_count = 0
cached_content_token_count = 0
tool_use_prompt_token_count = 0
total_token_count = 0
def accumulate_token_count(usage_metadata):
    global prompt_token_count, candidates_token_count, cached_content_token_count, tool_use_prompt_token_count, total_token_count
    prompt_token_count += usage_metadata.prompt_token_count
    candidates_token_count += usage_metadata.candidates_token_count
    cached_content_token_count += usage_metadata.cached_content_token_count if usage_metadata.cached_content_token_count else 0
    tool_use_prompt_token_count += usage_metadata.tool_use_prompt_token_count if usage_metadata.tool_use_prompt_token_count else 0
    total_token_count += usage_metadata.total_token_count
def cost():
    return round((prompt_token_count * price[model]['input'] + candidates_token_count * price[model]['output'])/1e6, 2)

# 1st API call
def get_user_prompt_type() -> str:
    system_prompt = """
    ç”¨æˆ¶æå•èˆ‡ä¸‹åˆ—é¸é …ä½•è€…æœ€ç›¸é—œï¼Ÿ
    1. ç¸½é«”ç¶“æ¿Ÿã€è²¡ç¶“è³‡è¨Šã€é‡‘èå¸‚å ´ç­‰ç›¸é—œçŸ¥è­˜æˆ–æ™‚äº‹
    2. è²¡ç¶“Må¹³æ–¹å®¢æˆ¶æœå‹™ã€å•†å‹™åˆä½œ
    3. å…¶ä»–
    å›å‚³æ•¸å­—ï¼Œç„¡å…¶ä»–æ–‡å­—ã€ç¬¦è™Ÿ
    """
    try:
        response = client.models.generate_content(
            model=model,
            contents=st.session_state.contents[-2:],
            config=GenerateContentConfig(
                system_instruction=system_prompt,
                response_mime_type="text/plain",
            )
        )
        result = response.text
        accumulate_token_count(response.usage_metadata)
    except Exception as e:
        st.code(f"Errrr: {e}")
        result = '3'
    finally:
        # MUST strip to remove \n
        result = result.strip()
        st.code({'1': 'ç”¨æˆ¶æå•ä¸»è¦é—œæ–¼è²¡ç¶“', '2': 'ç”¨æˆ¶æå•ä¸»è¦é—œæ–¼å®¢æœ', '3': 'ç”¨æˆ¶æå•èˆ‡è²¡ç¶“æˆ–å®¢æœç„¡é—œ'}[result])
        return result

# 2nd ~ 6th API calls
def get_relevant_ids(csv_df_json) -> str:
    system_prompt = 'Given a user query, identify up to 5 of the most relevant IDs in the JSON below. Output only the IDs, with no additional text.\n'
    system_prompt += st.session_state.knowledge[csv_df_json]
    try:
        response = client.models.generate_content(
            model=model,
            contents=user_prompt,
            config=GenerateContentConfig(
                system_instruction=system_prompt,
                response_mime_type="application/json",
            )
        )
        result = response.text
        accumulate_token_count(response.usage_metadata)
    except Exception as e:
        st.code(f"Errrr: {e}")
        result = '[]'
    finally:
        st.code(csv_df_json.replace('df.iloc[:,:2].to_json', result))
        return result

def get_retrieval(csv_file) -> str:
    try:
        ids = json.loads(get_relevant_ids(csv_file + ' => df.iloc[:,:2].to_json'))
    except json.JSONDecodeError as e:
        st.code(f"JSONDecodeError: {e}")
        ids = None
    if ids:
        if type(ids[0]) is dict:
            ids = [int(_id['id']) for _id in ids]
        else:
            ids = [int(_id) for _id in ids]

        if user_prompt_type == '1':
            df = st.session_state.knowledge[csv_file]
            df = df[df['id'].isin(ids)]
        if user_prompt_type == '2':
            df = pd.DataFrame(columns=['id', 'html'])
            df['id'] = ids
            htmls = []
            for _id in ids:
                with open(csv_file.replace('_log', str(_id)).replace('csv', 'html')) as f:
                    htmls.append(''.join(f.readlines()))
            df['html'] = htmls
        return df.to_json(orient='records', force_ascii=False)
    else:
        return ''

site_languages = [
    'ç¹é«”ä¸­æ–‡',
    'ç®€ä½“ä¸­æ–‡',
    'English']
subheader_texts = [
    "è²¡ç¶“æ™‚äº‹ç›¸é—œå•é¡Œï¼Œä¾‹å¦‚ï¼šç¾å‚µæ®–åˆ©ç‡ç‚ºä½•é£†å‡ï¼Ÿ",
    "è´¢ç»æ—¶äº‹ç›¸å…³é—®é¢˜ï¼Œä¾‹å¦‚ï¼šç¾å€ºæ”¶ç›Šç‡ä¸ºä½•é£™å‡ï¼Ÿ",
    "Financial and economic questions, e.g.: Why are US Treasury yields surging?"]
subdomains = [
    'www',
    'sc',
    'en']
lang_routes = [
    'zh-tw',
    'zh-cn',
    'en-001']

with st.sidebar:
    st.title('ğŸ‘©ğŸ»â€ğŸ’¼ MM Madam')
    st.link_button('ç³»çµ±æç¤ºè©å…±ç­†ï¼ŒåŸå‰‡åªå¢ä¸åˆªï¼Œå¦‚éœ€åˆªé™¤è«‹ä»¥è¨»è§£æ–¹å¼èªªæ˜åŸå› ï¼Œç·¨è¼¯åŒæ™‚å•ç­”ç«‹å³ç”Ÿæ•ˆï¼Œç„¡éœ€é‡æ–°æ•´ç†æ­¤ç¶²é ', 'https://docs.google.com/document/d/1HOS7nntBTgfuSlUpHgDIfBed5M_bq4dH0H8kqXUO9PE/edit?usp=sharing', icon='ğŸ“')
    st.link_button('è«‹å”åŠ©ä½¿ç”¨å„ªåŒ–éçš„ç³»çµ±æç¤ºè©ï¼Œå°é¡Œåº«é€²è¡Œä¸€è¼ªå¯¦æ¸¬ï¼Œè¤‡è£½è²¼ä¸ŠAIç”Ÿæˆç­”è¦†ï¼Œæä¾›AIå°ˆæ¡ˆæœƒè­°è¨è«–', 'https://docs.google.com/spreadsheets/d/1pe3d54QEyU0xQ_vJe_308UK9FzLYQJl7EQZkSyYgLeA/edit?usp=sharing', icon='ğŸ’¬')
    st.markdown('---')
    site_language = st.radio('ç¶²ç«™èªç³»', site_languages, horizontal=True)
    is_paid_user = st.toggle('ğŸ’ ä»˜è²»ç”¨æˆ¶', value=True)
    has_chart = st.toggle('ğŸ“Š MMåœ–è¡¨', value=is_paid_user, disabled=not is_paid_user)
    has_quickie = st.toggle(f'ğŸ’¡ MMçŸ­è©•', value=is_paid_user, disabled=not is_paid_user)
    has_blog = st.toggle(f'ğŸ“ MMéƒ¨è½æ ¼', value=is_paid_user, disabled=not is_paid_user)
    has_edm = st.toggle(f'ğŸ“® MMç¨å®¶å ±å‘Š', value=is_paid_user, disabled=not is_paid_user)
    has_stocks = st.toggle('ğŸ“ˆ MMç¾è‚¡è²¡å ±è³‡æ–™åº«', value=True)
    has_hc = st.toggle('â“ MMå¹«åŠ©ä¸­å¿ƒ', value=True)
    has_search = st.toggle('ğŸ” Googleæœå°‹', value=True)
    has_memory = st.toggle('ğŸ§  è¨˜å¾—å‰äº”æ¬¡å•ç­”', value=False)
    st.markdown('---')
    model = st.selectbox('Model', price.keys())

if has_memory:
    # include and display the last 5 turns of conversation before the current turn
    st.session_state.contents = st.session_state.contents[-10:]
    for content in st.session_state.contents:
        with st.chat_message(content.role, avatar=None if content.role == "user" else 'ğŸ‘©ğŸ»â€ğŸ’¼'):
            st.markdown(content.parts[0].text)
else:
    # initialize the conversation history when has_memory defaults to False
    # clear the conversation history
    st.session_state.contents = []

def get_started():
    st.session_state.get_started = ...
if 'get_started' not in st.session_state:
    with st.container():
        subheader_text = dict(zip(site_languages, subheader_texts))[site_language]
        st.subheader(subheader_text)
        user_prompt = st.chat_input('Ask Madam', on_submit=get_started)
else:
    client = genai.Client(api_key=st.secrets['GEMINI_API_KEY'])
    # When st.chat_input is used in the main body of an app, it will be pinned to the bottom of the page.
    user_prompt = st.chat_input('Ask Madam')

if 'knowledge' not in st.session_state:
    st.session_state.knowledge = {}
    for csv_file in glob.glob('knowledge/*.csv') + glob.glob('knowledge/*/*/*.csv'):
        df = pd.read_csv(csv_file)
        # quickie, blog, edm
        if 'date' in df.columns:
            df = df[df['date'] > after]
        st.session_state.knowledge[csv_file] = df
        st.session_state.knowledge[csv_file + ' => df.iloc[:,:2].to_json'] = df.iloc[:,:2].to_json(orient='records', force_ascii=False)

system_prompt = requests.get(st.secrets['SYSTEM_PROMPT_URL']).text
if user_prompt:
    with st.chat_message("user"):
        st.markdown(user_prompt)
    st.session_state.contents.append(Content(role="user", parts=[Part.from_text(text=user_prompt)]))

    user_prompt_type = get_user_prompt_type()
    if user_prompt_type == '1':
        subdomain = dict(zip(site_languages, subdomains))[site_language]
        if not is_paid_user:
            system_prompt += f'\n- ä½ æœƒé¼“å‹µç”¨æˆ¶å‡ç´šæˆç‚ºä»˜è²»ç”¨æˆ¶å°±èƒ½äº«æœ‰å®Œæ•´å•ç­”æœå‹™ï¼Œä¸¦ä¸”æä¾›è¨‚é–±æ–¹æ¡ˆé€£çµ https://{subdomain}.macromicro.me/subscribe'
        if has_chart:
            if retrieval := get_retrieval(glob.glob('knowledge/chart-*.csv')[0]):
                system_prompt += f'\n- MMåœ–è¡¨çš„è³‡æ–™ï¼Œç•¶ä¸­æ™‚é–“åºåˆ—æœ€æ–°å…©ç­†æ•¸æ“šï¼ˆseries_last_rowsï¼‰å¾ˆé‡è¦ï¼Œå‹™å¿…å¼•ç”¨\n```{retrieval}```'
                system_prompt += f'\nç¶²å€è¦å‰‡ https://{subdomain}.macromicro.me/charts/{{id}}/{{slug}}'
        if has_quickie and site_language in site_languages[:2]:
            if retrieval := get_retrieval(glob.glob('knowledge/quickie-*.csv')[0]):
                system_prompt += f'\n- MMçŸ­è©•çš„è³‡æ–™\n```{retrieval}```'
                system_prompt += f'\nç¶²å€è¦å‰‡ https://{subdomain}.macromicro.me/quickie?id={{id}}'
        if has_blog and site_language in site_languages[:2]:
            if retrieval := get_retrieval(glob.glob('knowledge/blog-*.csv')[0]):
                system_prompt += f'\n- MMéƒ¨è½æ ¼çš„è³‡æ–™\n```{retrieval}```'
                system_prompt += f'\nç¶²å€è¦å‰‡ https://{subdomain}.macromicro.me/blog/{{slug}}'
        if has_blog and site_language == 'English':
            if retrieval := get_retrieval(glob.glob('knowledge/blog_en-*.csv')[0]):
                system_prompt += f'\n- MMéƒ¨è½æ ¼çš„è³‡æ–™\n```{retrieval}```'
                system_prompt += f'\nç¶²å€è¦å‰‡ https://{subdomain}.macromicro.me/blog/{{slug}}'
        if has_edm and site_language in site_languages[:2]:
            if retrieval := get_retrieval(glob.glob('knowledge/edm-*.csv')[0]):
                system_prompt += f'\n- MMç¨å®¶å ±å‘Šçš„è³‡æ–™\n```{retrieval}```'
                system_prompt += f'\nç¶²å€è¦å‰‡ https://{subdomain}.macromicro.me/mails/edm/{'tc' if site_language[0] == 'ç¹' else 'sc'}/display/{{id}}'
        if has_stocks:
            system_prompt += f'\n- è‹¥ç”¨æˆ¶æˆ–ä½ æåŠç¾åœ‹ä¸Šå¸‚å…¬å¸ï¼Œä½ æœƒæä¾›MMç¾è‚¡è²¡å ±è³‡æ–™åº«ä¸­è©²å…¬å¸çš„ç¶²é  https://{subdomain}.macromicro.me/stocks/info/{{è‚¡ç¥¨ä»£è™Ÿ}}'
        if has_search:
            try:
                response = client.models.generate_content(
                    model=model,
                    contents=user_prompt,
                    config=GenerateContentConfig(
                        tools=[Tool(google_search=GoogleSearch())],
                        response_mime_type="text/plain",
                    ),
                )
                result = response.text
                accumulate_token_count(response.usage_metadata)
            except Exception as e:
                st.code(f"Errrr: {e}")
                result = ''
            finally:
                if retrieval := result:
                    system_prompt += f'\n- ç¶²è·¯æœå°‹çš„è³‡æ–™\n```{retrieval}```'
    if user_prompt_type == '2':
        if has_hc:
            lang_route = dict(zip(site_languages, lang_routes))[site_language]
            if retrieval := get_retrieval(f'knowledge/hc/{lang_route}/_log.csv'):
                system_prompt += f'\n- MMå¹«åŠ©ä¸­å¿ƒçš„è³‡æ–™\n```{retrieval}```'
                system_prompt += f'\nç¶²å€è¦å‰‡ https://support.macromicro.me/hc/{lang_route}/articles/{{id}}'
            else:
                system_prompt += '\n- MMå¹«åŠ©ä¸­å¿ƒç„¡ç›¸é—œè³‡æ–™ï¼Œè«‹ç”¨æˆ¶ä¾†ä¿¡ support@macromicro.me'
        else:
            system_prompt += '\n- MMå¹«åŠ©ä¸­å¿ƒç„¡ç›¸é—œè³‡æ–™ï¼Œè«‹ç”¨æˆ¶ä¾†ä¿¡ support@macromicro.me'
    if user_prompt_type == '3':
        system_prompt += '\n- è‹¥éè²¡ç¶“æ™‚äº‹ç›¸é—œå•é¡Œï¼Œä½ æœƒå©‰æ‹’å›ç­”'
    st.code(system_prompt)
    # st.markdown(system_prompt)
    try:
        response = client.models.generate_content(
            model=model,
            contents=st.session_state.contents,
            config=GenerateContentConfig(
                system_instruction=system_prompt,
                response_mime_type="text/plain",
            ),
        )
        result = response.text
        accumulate_token_count(response.usage_metadata)
    except Exception as e:
        st.code(f"Errrr: {e}")
        result = 'æŠ±æ­‰ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚ã€‚ã€‚'
    finally:
        with st.chat_message("assistant", avatar='ğŸ‘©ğŸ»â€ğŸ’¼'):
            st.markdown(result)
        st.session_state.contents.append(Content(role="model", parts=[Part.from_text(text=result)]))

        st.badge(f'{prompt_token_count} input tokens + {candidates_token_count} output tokens â‰’ {cost()} USD ( when Google Search < 1500 Requests/Day )', icon="ğŸ’°", color="green")

        hackmd_note_api = st.secrets['HACKMD_NOTE_API']
        headers = {f"Authorization": "Bearer {st.secrets['HACKMD_API_TOKEN']}"}
        r = requests.get(hackmd_note_api, headers=headers)
        if r.status_code == 200:
            payload = r.json()['content'] + '\n\n---\n\n'
            payload += st.session_state.contents[-2]['parts'][0]['text'] + '\n\n---\n\n' + result
            r = requests.patch(hackmd_note_api, headers=headers, json=payload)