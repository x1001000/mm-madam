import streamlit as st
from openai import OpenAI

import glob
csvs = glob.glob('*.csv')

import os
models = {
    'gemini-2.0-flash': {
        'api_key': os.environ.get('GEMINI_API_KEY'),
        'base_url': "https://generativelanguage.googleapis.com/v1beta/openai/"
        },
    'gemini-2.0-flash-lite': {
        'api_key': os.environ.get('GEMINI_API_KEY'),
        'base_url': "https://generativelanguage.googleapis.com/v1beta/openai/"
        },
    # 'grok-2': {
    #     'api_key': os.environ.get('XAI_API_KEY'),
    #     'base_url': "https://api.x.ai/v1"
    #     },
    }

st.title('ğŸ‘©ğŸ»â€ğŸ’¼ MM Madame')

col1, col2 = st.columns(2)
with col1:
    csv = st.selectbox("çŸ¥è­˜åº«", csvs)
with col2:
    model = st.selectbox("èªè¨€æ¨¡å‹", models.keys())

# Create session state variables
if 'client' not in st.session_state:
    st.session_state.client = OpenAI(**models[model])
    st.session_state.messages = []
    st.session_state.knowledge = {}
    for csv in csvs:
        with open(csv) as f:
            st.session_state.knowledge[csv] = ''.join(f.readlines())

retrieval_prompt = 'ä½¿ç”¨è€…æå•èˆ‡ä¸‹æ–¹è³‡æ–™è¡¨ä¸­æœ‰é—œçš„idï¼Œè¼¸å‡ºæˆJSON\n\n\n' + st.session_state.knowledge[csv]

# Display the existing chat messages via `st.chat_message`.
for message in st.session_state.messages:
    with st.chat_message(message["role"], avatar=None if message["role"] == "user" else 'ğŸ‘©ğŸ»â€ğŸ’¼'):
        st.markdown(message["content"])

# Create a chat input field to allow the user to enter a message. This will display
# automatically at the bottom of the page.
if user_prompt := st.chat_input("å•æˆ‘ç¸½ç¶“ç›¸é—œçš„å•é¡Œå§"):

    # Store and display the current user_prompt.
    st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    # Last 5 rounds of conversation queued
    st.session_state.messages = st.session_state.messages[-11:]
    # Generate a response using the OpenAI API.
    response = st.session_state.client.chat.completions.create(
        model=model,
        messages=[
            {'role': 'system', 'content': retrieval_prompt},
            ] + st.session_state.messages,
        # stream=True,
        response_format={"type": "json_object"},
    )

    system_prompt = 'å¦³æ˜¯ç¸½ç¶“æŠ•è³‡å¹³å°ã€Œè²¡ç¶“Må¹³æ–¹ï¼ˆMacroMicroï¼‰ã€çš„AIç ”ç©¶å“¡ï¼šMadameã€‚å¦³æœƒä¾æ“šå¹³å°çŸ¥è­˜åº«æœå°‹çµæœå›ç­”å•é¡Œï¼Œä¸¦æä¾›åœ–è¡¨é€£çµï¼ˆhttps://www.macromicro.me/charts/{id}/{slug}ï¼‰ã€‚è‹¥éç¸½ç¶“ç›¸é—œå•é¡Œï¼Œå¦³æœƒå‘ŠçŸ¥ä¸ä¾¿å›ç­”ã€‚\n\næœå°‹çµæœå¦‚ä¸‹ï¼š\n' + response.choices[0].message.content
    print(system_prompt)
    stream = st.session_state.client.chat.completions.create(
        model=model,
        messages=[
            {'role': 'system', 'content': system_prompt},
            ] + st.session_state.messages,
        stream=True,
    )
    # Stream the response to the chat using `st.write_stream`, then store it in 
    # session state.
    with st.chat_message("assistant", avatar='ğŸ‘©ğŸ»â€ğŸ’¼'):
        response = st.write_stream(stream)
    st.session_state.messages.append({"role": "assistant", "content": response})
