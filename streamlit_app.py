import streamlit as st
from openai import OpenAI

import glob
csvs = glob.glob('*.csv')

import os
models = {
    'gemini-2.0-flash': {
        'api_key': os.environ.get('GEMINI_API_KEY'),
        'base_url': "https://generativelanguage.googleapis.com/v1beta/openai/"
        },
    'gemini-2.0-flash-lite': {
        'api_key': os.environ.get('GEMINI_API_KEY'),
        'base_url': "https://generativelanguage.googleapis.com/v1beta/openai/"
        },
    # 'grok-2': {
    #     'api_key': os.environ.get('XAI_API_KEY'),
    #     'base_url': "https://api.x.ai/v1"
    #     },
    }

st.title('ğŸ‘©ğŸ»â€ğŸ’¼ MM Madame')

col1, col2 = st.columns(2)
with col1:
    csv = st.selectbox("çŸ¥è­˜åº«", csvs)
with col2:
    model = st.selectbox("èªè¨€æ¨¡å‹", models.keys())

# Create session state variables
if 'client' not in st.session_state:
    st.session_state.client = OpenAI(**models[model])
    st.session_state.messages = []
    st.session_state.knowledge = {}
    for csv in csvs:
        with open(csv) as f:
            st.session_state.knowledge[csv] = ''.join(f.readlines())

system_prompt = 'å¦³æ˜¯ç¸½ç¶“æŠ•è³‡å¹³å°ã€Œè²¡ç¶“Må¹³æ–¹ï¼ˆMacroMicroï¼‰ã€çš„AIç ”ç©¶å“¡ï¼šMadameã€‚å¦³æœƒä¾æ“šå¹³å°çŸ¥è­˜åº«å›ç­”å•é¡Œï¼Œä¸¦ä¸”æ¨™è¨»å‡ºè™•idã€‚è‹¥éç¸½ç¶“ç›¸é—œå•é¡Œï¼Œå¦³æœƒå‘ŠçŸ¥ä¸ä¾¿å›ç­”ã€‚\n\n' + st.session_state.knowledge[csv]
# print(system_prompt)

# Display the existing chat messages via `st.chat_message`.
for message in st.session_state.messages:
    with st.chat_message(message["role"], avatar=None if message["role"] == "user" else 'ğŸ‘©ğŸ»â€ğŸ’¼'):
        st.markdown(message["content"])

# Create a chat input field to allow the user to enter a message. This will display
# automatically at the bottom of the page.
if user_prompt := st.chat_input("å•æˆ‘ç¸½ç¶“ç›¸é—œçš„å•é¡Œå§"):

    # Store and display the current user_prompt.
    st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    # Last 5 rounds of conversation queued before the current_time/user_prompt.
    st.session_state.messages = st.session_state.messages[-10:]
    # Generate a response using the OpenAI API.
    stream = st.session_state.client.chat.completions.create(
        model=model,
        messages=[
            {'role': 'system', 'content': system_prompt},
            ] + st.session_state.messages,
        stream=True,
    )

    # Stream the response to the chat using `st.write_stream`, then store it in 
    # session state.
    with st.chat_message("assistant", avatar='ğŸ‘©ğŸ»â€ğŸ’¼'):
        response = st.write_stream(stream)
    st.session_state.messages.append({"role": "assistant", "content": response})
