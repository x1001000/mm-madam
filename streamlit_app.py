import streamlit as st
from google import genai
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch, Content, Part
import pandas as pd
import glob
csvs = glob.glob('*.csv')
models = [
    'gemini-2.0-flash',
    'gemini-2.0-flash-lite',
]

def is_economics_related() -> bool:
    """
    Determine if a question is related to economics or finance.
    
    Args:
        client: Gemini API client
        question: User's question to evaluate
        model: Gemini model name to use
        
    Returns:
        bool: True if question is economics/finance related, False otherwise
    """
    
    system_prompt = """
    Determine if the input question is related to:
    - Economics
    - Finance
    - Markets
    - Trading
    - Banking
    - Monetary policy
    - Fiscal policy
    - Economic indicators
    
    Output only 'true' or 'false'. No other text.
    """
    
    try:
        response = client.models.generate_content(
            model=model,
            contents=st.session_state.contents[-1:],
            config=GenerateContentConfig(
                system_instruction=system_prompt,
                response_mime_type="text/plain",
            )
        )
        
        # return response.text.strip().lower() == 'true'
        return 'true' in response.text.strip().lower()
        
    except Exception as e:
        print(f"Error checking question relevance: {e}")
        return False

def get_relevant_chart_ids() -> list[int]:
    """
    Get relevant MacroMicro chart IDs based on user question using Gemini model.
    
    Args:
        client: Gemini API client
        user_question: User's question about economic/financial topics
        model: Gemini model name to use
        
    Returns:
        list of relevant chart IDs
    """
    
    system_prompt = """
    Given a user question, identify relevant MacroMicro chart IDs.
    Return ONLY a comma-separated list of chart IDs. Do not include any other text.
    
    MacroMicro_charts.csv:
    """ + st.session_state.knowledge[csv]
    
    try:
        response = client.models.generate_content(
            model=model,
            contents=st.session_state.contents[-1:],
            config=GenerateContentConfig(
                system_instruction=system_prompt,
                response_mime_type="text/plain",
            )
        )
        
        # Extract chart IDs from response
        chart_ids = [
            int(chart_id.strip())
            for chart_id in response.text.split(',')
            if chart_id.strip().isdigit()
        ]
        
        return chart_ids
        
    except Exception as e:
        print(f"Error getting chart IDs: {e}")
        return []

st.title('ğŸ‘©ğŸ»â€ğŸ’¼ MM Madam')

col1, col2 = st.columns(2)
with col1:
    csv = st.selectbox("MacroMicro åœ–è¡¨è³‡æ–™", csvs)
with col2:
    model = st.selectbox("Gemini èªè¨€æ¨¡å‹", models)

# Add search toggle
enable_search = st.toggle("å•Ÿç”¨ç¶²è·¯æœå°‹", value=True, help="é–‹å•Ÿå¾Œï¼ŒMM Madam å°‡ä½¿ç”¨ Google æœå°‹ä¾†è¼”åŠ©å›ç­”")

# Create session state variables
if 'client' not in st.session_state:
    st.session_state.client = genai.Client(api_key=st.secrets['GEMINI_API_KEY'])
    st.session_state.contents = []
    st.session_state.knowledge = {}
    for csv in csvs:
        with open(csv) as f:
            st.session_state.knowledge[csv] = ''.join(f.readlines())
        st.session_state.knowledge['DataFrame of '+csv] = pd.read_csv(csv)
client = st.session_state.client

# include and display the last 5 turns of conversation before the current turn
st.session_state.contents = st.session_state.contents[-10:]
for content in st.session_state.contents:
    with st.chat_message(content.role, avatar=None if content.role == "user" else 'ğŸ‘©ğŸ»â€ğŸ’¼'):
        st.markdown(content.parts[0].text)

# Create a chat input field to allow the user to enter a message. This will display
# automatically at the bottom of the page.
if user_prompt := st.chat_input("è²¡ç¶“æ™‚äº‹ç›¸é—œå•é¡Œï¼Œä¾‹å¦‚ï¼šå·æ™®é—œç¨…æœ€æ–°é€²å±•èˆ‡å°å°è¡æ“Šï¼Ÿ"):
    with st.chat_message("user"):
        st.markdown(user_prompt)
    st.session_state.contents.append(Content(role="user", parts=[Part.from_text(text=user_prompt)]))

    system_prompt = 'å¦³æ˜¯ã€Œè²¡ç¶“Må¹³æ–¹ï¼ˆMacroMicroï¼‰ã€çš„AIç ”ç©¶å“¡ï¼šMadamï¼Œå¦³ç›®å‰çš„å·¥ä½œæ˜¯å›ç­”è²¡ç¶“æ™‚äº‹ç›¸é—œå•é¡Œã€‚'
    if is_economics_related():
        df = st.session_state.knowledge['DataFrame of '+csv]
        if chart_ids := get_relevant_chart_ids():
            retrieval = df[df['id'].isin(chart_ids)].to_csv(index=False, quoting=1)
            system_prompt += '''å¦³æœƒä¾æ“šMacroMicroåœ–è¡¨è³‡æ–™åŠGoogleæœå°‹çµæœå›ç­”å•é¡Œï¼Œä¸¦ä¸”æä¾›MacroMicroåœ–è¡¨è¶…é€£çµ https://www.macromicro.me/charts/{id}/{slug} ï¼Œè¶…é€£çµå‰å¾Œç©ºæ ¼æˆ–æ›è¡Œã€‚
            
            MacroMicroåœ–è¡¨è³‡æ–™.csv å¦‚ä¸‹\n''' + retrieval
            print(chart_ids)
    else:
        system_prompt += 'è‹¥éè²¡ç¶“æ™‚äº‹ç›¸é—œå•é¡Œï¼Œå¦³æœƒå©‰æ‹’å›ç­”ã€‚'
    print(system_prompt)
    response = client.models.generate_content(
        model=model,
        contents=st.session_state.contents,
        config=GenerateContentConfig(
            system_instruction=system_prompt,
            tools=[Tool(google_search=GoogleSearch())] if enable_search else None,
            response_mime_type="text/plain",
        ),
    )
    # Stream the response to the chat using `st.write_stream`, then store it in 
    # session state.
    with st.chat_message("assistant", avatar='ğŸ‘©ğŸ»â€ğŸ’¼'):
        st.markdown(response.text)
    st.session_state.contents.append(Content(role="model", parts=[Part.from_text(text=response.text)]))