import streamlit as st
from google import genai
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch, Content, Part
import pandas as pd
import json
import glob
import re

def get_user_prompt_type() -> str:
    system_prompt = """
    ç”¨æˆ¶æå•èˆ‡ä¸‹åˆ—é¸é …ä½•è€…æœ€ç›¸é—œï¼Ÿ
    1. ç¸½é«”ç¶“æ¿Ÿã€è²¡ç¶“è³‡è¨Šã€é‡‘èå¸‚å ´ç­‰ç›¸é—œçŸ¥è­˜æˆ–æ™‚äº‹
    2. è²¡ç¶“Må¹³æ–¹å®¢æˆ¶æœå‹™ã€å•†å‹™åˆä½œ
    3. å…¶ä»–
    å›å‚³æ•¸å­—ï¼Œç„¡å…¶ä»–æ–‡å­—ã€ç¬¦è™Ÿ
    """
    response = client.models.generate_content(
        model=model,
        contents=st.session_state.contents[-1:],
        config=GenerateContentConfig(
            system_instruction=system_prompt,
            response_mime_type="text/plain",
        )
    )
    return response.text.strip()

def get_relevant_ids_json(csv) -> str:
    system_prompt = 'Given a user question, identify relevant records in the CSV file, output only ids\n\n'
    system_prompt += st.session_state.knowledge[csv]
    response = client.models.generate_content(
        model=model,
        contents=st.session_state.contents[-1:],
        config=GenerateContentConfig(
            system_instruction=system_prompt,
            response_mime_type="application/json",
        )
    )
    print(csv, response.text)
    return response.text

def initialize_client():
    st.session_state.client = genai.Client(api_key=st.secrets['GEMINI_API_KEY'])

if 'client' not in st.session_state:
    st.session_state.contents = []
    st.session_state.knowledge = {}
    for csv in glob.glob('data/*.csv'):
        with open(csv) as f:
            st.session_state.knowledge[csv] = ''.join(f.readlines())
        st.session_state.knowledge['DataFrame of '+csv] = pd.read_csv(csv)
    with st.container():
        st.subheader("è²¡ç¶“æ™‚äº‹ç›¸é—œå•é¡Œï¼Œä¾‹å¦‚ï¼šç¾å‚µæ®–åˆ©ç‡ç‚ºä½•é£†é«˜ï¼Ÿ")
        user_prompt = st.chat_input('Ask Madam', on_submit=initialize_client)
else:
    client = st.session_state.client
    user_prompt = st.chat_input('Ask Madam')

with st.sidebar:
    st.title('ğŸ‘©ğŸ»â€ğŸ’¼ MM Madam')
    has_chart = st.toggle('ğŸ“Š MMåœ–è¡¨', value=True)
    has_quickie = st.toggle('ğŸ’¡ MMçŸ­è©•', value=True)
    has_blog = st.toggle('ğŸ“ MMéƒ¨è½æ ¼', value=True)
    has_edm = st.toggle('ğŸ“® MMç¨å®¶å ±å‘Š', value=True)
    has_search = st.toggle('ğŸ” Googleæœå°‹', value=True)
    model = st.selectbox('Model', ['gemini-2.0-flash', 'gemini-2.5-flash-preview-04-17'])

# include and display the last 5 turns of conversation before the current turn
st.session_state.contents = st.session_state.contents[-10:]
for content in st.session_state.contents:
    with st.chat_message(content.role, avatar=None if content.role == "user" else 'ğŸ‘©ğŸ»â€ğŸ’¼'):
        st.markdown(content.parts[0].text)

# Create a chat input field to allow the user to enter a message. This will display
# automatically at the bottom of the page.
if user_prompt:
    with st.chat_message("user"):
        st.markdown(user_prompt)
    st.session_state.contents.append(Content(role="user", parts=[Part.from_text(text=user_prompt)]))

    system_prompt = '# å¦³æ˜¯ã€Œè²¡ç¶“Må¹³æ–¹ï¼ˆMacroMicroï¼‰ã€çš„AIç ”ç©¶å“¡ï¼šMadamï¼Œå¦³æœƒæä¾›ç¸½é«”ç¶“æ¿Ÿã€è²¡ç¶“è³‡è¨Šã€é‡‘èå¸‚å ´ç­‰ç›¸é—œçŸ¥è­˜çš„ç§‘æ™®åŠå°ˆæ¥­å•ç­”ï¼Œå„˜é‡ä½¿ç”¨Markdownè¡¨æ ¼é€²è¡Œè«–è¿°ï¼Œç•¶æåŠã€è²¡ç¶“Må¹³æ–¹ã€æˆ–ã€MacroMicroã€æ™‚ï¼Œå‹™å¿…ä½¿ç”¨ã€æˆ‘å€‘ã€ã€‚\n'
    user_prompt_type = get_user_prompt_type()
    if user_prompt_type == '1':
        if has_chart:
            csv = glob.glob('data/chart*.csv')[-1]
            ids = json.loads(get_relevant_ids_json(csv))
            ids = [int(id_) for id_ in ids if id_.isdigit()]
            df = st.session_state.knowledge['DataFrame of '+csv]
            retrieval_dict = df[df['id'].isin(ids)].to_dict(orient='records')
            system_prompt += '\n\n## å¦³æœƒä¾æ“šä»¥ä¸‹MMåœ–è¡¨çš„è³‡æ–™å›ç­”å•é¡Œï¼Œä¸¦ä¸”æä¾›MMåœ–è¡¨è¶…é€£çµ https://www.macromicro.me/charts/{id}/{slug} ï¼Œè¶…é€£çµå‰å¾Œè¦ç©ºæ ¼æˆ–æ›è¡Œã€‚\n'
            system_prompt += json.dumps(retrieval_dict, ensure_ascii=False)
        if has_quickie:
            csv = glob.glob('data/quickie*.csv')[-1]
            ids = json.loads(get_relevant_ids_json(csv))[:1]
            ids = [int(id_) for id_ in ids if id_.isdigit()]
            df = st.session_state.knowledge['DataFrame of '+csv]
            retrieval_dict = df[df['id'].isin(ids)].to_dict(orient='records')
            system_prompt += '\n\n## å¦³æœƒä¾æ“šä»¥ä¸‹MMçŸ­è©•çš„è³‡æ–™å›ç­”å•é¡Œï¼Œä¸¦ä¸”æä¾›MMçŸ­è©•è¶…é€£çµ https://www.macromicro.me/quickie?id={id} ï¼Œè¶…é€£çµå‰å¾Œè¦ç©ºæ ¼æˆ–æ›è¡Œã€‚\n'
            system_prompt += json.dumps(retrieval_dict, ensure_ascii=False)
        if has_blog:
            csv = glob.glob('data/blog*.csv')[-1]
            ids = json.loads(get_relevant_ids_json(csv))[:1]
            ids = [int(id_) for id_ in ids if id_.isdigit()]
            df = st.session_state.knowledge['DataFrame of '+csv]
            retrieval_dict = df[df['id'].isin(ids)].to_dict(orient='records')
            system_prompt += '\n\n## å¦³æœƒä¾æ“šä»¥ä¸‹MMéƒ¨è½æ ¼çš„è³‡æ–™å›ç­”å•é¡Œï¼Œä¸¦ä¸”æä¾›MMéƒ¨è½æ ¼è¶…é€£çµ https://www.macromicro.me/blog/{slug} ï¼Œè¶…é€£çµå‰å¾Œè¦ç©ºæ ¼æˆ–æ›è¡Œã€‚\n'
            system_prompt += json.dumps(retrieval_dict, ensure_ascii=False)
        if has_edm:
            csv = glob.glob('data/edm*.csv')[-1]
            ids = json.loads(get_relevant_ids_json(csv))[:1]
            ids = [int(id_) for id_ in ids if id_.isdigit()]
            df = st.session_state.knowledge['DataFrame of '+csv]
            retrieval_dict = df[df['id'].isin(ids)].to_dict(orient='records')
            system_prompt += '\n\n## å¦³æœƒä¾æ“šä»¥ä¸‹MMç¨å®¶å ±å‘Šçš„è³‡æ–™å›ç­”å•é¡Œã€‚\n'
            system_prompt += json.dumps(retrieval_dict, ensure_ascii=False)
        if has_search:
            system_prompt += '\n\n## å¦³æœ€çµ‚æœƒä»¥Googleæœå°‹åšç‚ºäº‹å¯¦ä¾æ“šã€‚'
    if user_prompt_type == '2':
        system_prompt += '\n\n## å¦³æœƒæä¾›è²¡ç¶“Må¹³æ–¹çš„å®¢æˆ¶æœå‹™ã€å•†å‹™åˆä½œç­‰ç›¸é—œè³‡è¨Šã€‚'
    if user_prompt_type == '3':
        system_prompt += '\n\n## è‹¥éè²¡ç¶“æ™‚äº‹ç›¸é—œå•é¡Œï¼Œå¦³æœƒå©‰æ‹’å›ç­”ã€‚'
    print(system_prompt)
    response = client.models.generate_content(
        model=model,
        contents=st.session_state.contents,
        config=GenerateContentConfig(
            tools=[Tool(google_search=GoogleSearch())] if has_search else None,
            system_instruction=system_prompt,
            response_mime_type="text/plain",
        ),
    )
    # remove reference markers
    response_text = re.sub(r'\[\d+\]', '', response.text)
    with st.chat_message("assistant", avatar='ğŸ‘©ğŸ»â€ğŸ’¼'):
        st.markdown(response_text)
    st.session_state.contents.append(Content(role="model", parts=[Part.from_text(text=response_text)]))